{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## NCCN Guideline Chat Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b38ad1-d90d-4808-99b8-d4c513d7a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF\n",
    "#!pip install pdfplumber\n",
    "#%pip install langchain\n",
    "#%pip install langchain-community\n",
    "#%pip install langchain-openai\n",
    "#%pip install langchain-chroma\n",
    "#%pip install fastapi\n",
    "#%pip install pypdf\n",
    "#%pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ff95c-0e6e-4da6-9c14-430bfdc65528",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import langchain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "#import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, PDFPlumberLoader, UnstructuredPDFLoader, PDFMinerLoader\n",
    "from pypdf.errors import PdfReadError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b1b8f5-bfde-4da4-a888-be84fef17484",
   "metadata": {},
   "source": [
    "### Load the API keys for OpenAI and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../keys.env\")\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['HF_API_KEY'] = os.getenv(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5499950-efe2-48f2-970c-a17182a24362",
   "metadata": {},
   "source": [
    "### Define the OpenAI LLM that will be used.  Set the name of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a low cost model from OpenAI will be sufficient\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"nccn_guidelines_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8b1ad-0eb9-4918-9322-4374133fa512",
   "metadata": {},
   "source": [
    "### Load in the PDF documents from the NCCN folder. A variety of PDF loaders will stand ready if there is a problem with one of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with PyPDFLoader for NCCN/cml2025.pdf\n",
      "Success with PyPDFLoader for NCCN/anal2025.pdf\n",
      "Success with PyPDFLoader for NCCN/biliary2025.pdf\n",
      "Success with PyPDFLoader for NCCN/myeloma2025.pdf\n",
      "Success with PyPDFLoader for NCCN/cervical2025.pdf\n",
      "Success with PyPDFLoader for NCCN/uveal2025.pdf\n",
      "Success with PyPDFLoader for NCCN/gist2025.pdf\n",
      "Success with PyPDFLoader for NCCN/mpn2025.pdf\n",
      "Success with PyPDFLoader for NCCN/cll_sll2025.pdf\n",
      "Success with PyPDFLoader for NCCN/mds2025.pdf\n",
      "Success with PyPDFLoader for NCCN/ovarian2025.pdf\n",
      "Success with PyPDFLoader for NCCN/hcc2025.pdf\n",
      "Success with PyPDFLoader for NCCN/neuroendocrine2025.pdf\n",
      "Success with PyPDFLoader for NCCN/squamous2025.pdf\n",
      "Success with PyPDFLoader for NCCN/colon2025.pdf\n",
      "Success with PyPDFLoader for NCCN/thyroid2025.pdf\n",
      "Success with PyPDFLoader for NCCN/small_bowel2024.pdf\n",
      "Success with PyPDFLoader for NCCN/uterine2025.pdf\n",
      "Success with PyPDFLoader for NCCN/kaposi2025.pdf\n",
      "Success with PyPDFLoader for NCCN/sclc2025.pdf\n",
      "Success with PyPDFLoader for NCCN/bladder2025.pdf\n",
      "Success with PyPDFLoader for NCCN/bone2025.pdf\n",
      "Success with PyPDFLoader for NCCN/nsclc2025.pdf\n",
      "Success with PyPDFLoader for NCCN/vaginal2025.pdf\n",
      "Success with PyPDFLoader for NCCN/esophageal2025.pdf\n",
      "Success with PyPDFLoader for NCCN/NHL_B-cell2025.pdf\n",
      "Success with PyPDFLoader for NCCN/cutaneous_melanoma2025.pdf\n",
      "Success with PyPDFLoader for NCCN/sarcoma2025.pdf\n",
      "Success with PyPDFLoader for NCCN/kidney2025.pdf\n",
      "Success with PyPDFLoader for NCCN/all2024.pdf\n",
      "Success with PyPDFLoader for NCCN/head-and-neck2025.pdf\n",
      "Success with PyPDFLoader for NCCN/aml2025.pdf\n",
      "Success with PyPDFLoader for NCCN/vulvar2024.pdf\n",
      "Success with PyPDFLoader for NCCN/gtn2025.pdf\n",
      "Success with PyPDFLoader for NCCN/waldenstroms2025.pdf\n",
      "Success with PyPDFLoader for NCCN/rectal2025.pdf\n",
      "Success with PyPDFLoader for NCCN/mcc2025.pdf\n",
      "Success with PyPDFLoader for NCCN/prostate2025.pdf\n",
      "Success with PyPDFLoader for NCCN/testicular2025.pdf\n",
      "Success with PyPDFLoader for NCCN/thymic2025.pdf\n",
      "Success with PyPDFLoader for NCCN/basal2025.pdf\n",
      "Success with PyPDFLoader for NCCN/breast2025.pdf\n",
      "Success with PyPDFLoader for NCCN/hodgkins2025.pdf\n",
      "Success with PyPDFLoader for NCCN/NHL_T-cell2025.pdf\n",
      "Success with PyPDFLoader for NCCN/histiocytic_neoplasms2025.pdf\n",
      "Success with PyPDFLoader for NCCN/pancreatic2025.pdf\n",
      "Success with PyPDFLoader for NCCN/ampullary2025.pdf\n",
      "Success with PyPDFLoader for NCCN/gastric2025.pdf\n",
      "Success with PyPDFLoader for NCCN/occult2025.pdf\n",
      "Success with PyPDFLoader for NCCN/cns2025.pdf\n",
      "PDF ingestion completed.\n"
     ]
    }
   ],
   "source": [
    "# Read in documents\n",
    "\n",
    "folder = \"NCCN\"  # Your folder containing PDFs\n",
    "\n",
    "pdf_files = [f for f in os.listdir(folder) if f.endswith('.pdf')]\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "def try_multiple_loaders(file_path):\n",
    "    loaders = [\n",
    "        (PyPDFLoader, \"PyPDFLoader\"),\n",
    "        (PDFPlumberLoader, \"PDFPlumberLoader\"),\n",
    "        (UnstructuredPDFLoader, \"UnstructuredPDFLoader\"),\n",
    "        (PDFMinerLoader, \"PDFMinerLoader\")\n",
    "    ]\n",
    "    \n",
    "    for LoaderClass, loader_name in loaders:\n",
    "        try:\n",
    "            loader = LoaderClass(file_path)\n",
    "            docs = loader.load()\n",
    "            print(f\"Success with {loader_name} for {file_path}\")\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            print(f\"{loader_name} failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return None  # If all loaders fail\n",
    "    \n",
    "\n",
    "documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    full_path = os.path.join(folder, pdf_file)\n",
    "    docs = try_multiple_loaders(full_path)\n",
    "    if docs:\n",
    "        doc_type = pdf_file  # or pdf_file.replace('.pdf', '') if you don't want the extension\n",
    "        documents.extend([add_metadata(doc, doc_type) for doc in docs])\n",
    "    else:\n",
    "        print(f\"All loaders failed for {pdf_file}\")\n",
    "\n",
    "print (\"PDF ingestion completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf82f7-7801-4dcc-9789-db80d0d8f222",
   "metadata": {},
   "source": [
    "### Here is where we define the chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a6ea82-e182-455c-9ed1-c33f93e66f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=60, separators=[\"\\n\\n\", \"\\n\", \".\", \", \"])\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3302552-b3c5-4f7e-9e25-c6c7a380d7e3",
   "metadata": {},
   "source": [
    "### Looks at the \"doc_type' of the metadata and identifies document types, such as PDF (or TXT). If PDF files were in the input stack, then one should see PDF files. This is quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4f700b-52de-421e-a863-540795248a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: gastric2025.pdf, esophageal2025.pdf, nsclc2025.pdf, pancreatic2025.pdf, head-and-neck2025.pdf, colon2025.pdf, kaposi2025.pdf, thymic2025.pdf, occult2025.pdf, uveal2025.pdf, mds2025.pdf, testicular2025.pdf, NHL_T-cell2025.pdf, hcc2025.pdf, small_bowel2024.pdf, thyroid2025.pdf, hodgkins2025.pdf, ovarian2025.pdf, ampullary2025.pdf, anal2025.pdf, myeloma2025.pdf, bone2025.pdf, histiocytic_neoplasms2025.pdf, biliary2025.pdf, basal2025.pdf, cml2025.pdf, rectal2025.pdf, breast2025.pdf, mcc2025.pdf, mpn2025.pdf, gist2025.pdf, aml2025.pdf, uterine2025.pdf, cns2025.pdf, cutaneous_melanoma2025.pdf, cervical2025.pdf, neuroendocrine2025.pdf, sclc2025.pdf, all2024.pdf, gtn2025.pdf, cll_sll2025.pdf, sarcoma2025.pdf, vaginal2025.pdf, NHL_B-cell2025.pdf, vulvar2024.pdf, kidney2025.pdf, squamous2025.pdf, bladder2025.pdf, waldenstroms2025.pdf, prostate2025.pdf\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1851b-e327-4f60-9688-3ed5a8c0d25d",
   "metadata": {},
   "source": [
    "### This code displays unique filenames instead of document types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d662396-ad58-45c9-ac3b-c830adda05c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed: gastric2025.pdf, esophageal2025.pdf, nsclc2025.pdf, pancreatic2025.pdf, head-and-neck2025.pdf, colon2025.pdf, kaposi2025.pdf, thymic2025.pdf, occult2025.pdf, uveal2025.pdf, mds2025.pdf, testicular2025.pdf, NHL_T-cell2025.pdf, hcc2025.pdf, small_bowel2024.pdf, thyroid2025.pdf, hodgkins2025.pdf, ovarian2025.pdf, ampullary2025.pdf, anal2025.pdf, myeloma2025.pdf, bone2025.pdf, histiocytic_neoplasms2025.pdf, biliary2025.pdf, basal2025.pdf, cml2025.pdf, rectal2025.pdf, breast2025.pdf, mcc2025.pdf, mpn2025.pdf, gist2025.pdf, aml2025.pdf, uterine2025.pdf, cns2025.pdf, cutaneous_melanoma2025.pdf, cervical2025.pdf, neuroendocrine2025.pdf, sclc2025.pdf, all2024.pdf, gtn2025.pdf, cll_sll2025.pdf, sarcoma2025.pdf, vaginal2025.pdf, NHL_B-cell2025.pdf, vulvar2024.pdf, kidney2025.pdf, squamous2025.pdf, bladder2025.pdf, waldenstroms2025.pdf, prostate2025.pdf\n",
      "Total unique files: 50\n"
     ]
    }
   ],
   "source": [
    "unique_files = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Files processed: {', '.join(unique_files)}\")\n",
    "print(f\"Total unique files: {len(unique_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a517df-6919-4932-becf-62a117eb38ec",
   "metadata": {},
   "source": [
    "### A more comprehensive summary of the loading and chunking of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e514f0-9c49-4f53-8fd3-79b29064f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDFs found: 50\n",
      "PDF files: cml2025.pdf, anal2025.pdf, biliary2025.pdf, myeloma2025.pdf, cervical2025.pdf, uveal2025.pdf, gist2025.pdf, mpn2025.pdf, cll_sll2025.pdf, mds2025.pdf, ovarian2025.pdf, hcc2025.pdf, neuroendocrine2025.pdf, squamous2025.pdf, colon2025.pdf, thyroid2025.pdf, small_bowel2024.pdf, uterine2025.pdf, kaposi2025.pdf, sclc2025.pdf, bladder2025.pdf, bone2025.pdf, nsclc2025.pdf, vaginal2025.pdf, esophageal2025.pdf, NHL_B-cell2025.pdf, cutaneous_melanoma2025.pdf, sarcoma2025.pdf, kidney2025.pdf, all2024.pdf, head-and-neck2025.pdf, aml2025.pdf, vulvar2024.pdf, gtn2025.pdf, waldenstroms2025.pdf, rectal2025.pdf, mcc2025.pdf, prostate2025.pdf, testicular2025.pdf, thymic2025.pdf, basal2025.pdf, breast2025.pdf, hodgkins2025.pdf, NHL_T-cell2025.pdf, histiocytic_neoplasms2025.pdf, pancreatic2025.pdf, ampullary2025.pdf, gastric2025.pdf, occult2025.pdf, cns2025.pdf\n",
      "\n",
      "Successfully processed documents: 50\n",
      "Sources: NCCN/cml2025.pdf, NCCN/gtn2025.pdf, NCCN/bladder2025.pdf, NCCN/sclc2025.pdf, NCCN/gist2025.pdf, NCCN/squamous2025.pdf, NCCN/nsclc2025.pdf, NCCN/rectal2025.pdf, NCCN/biliary2025.pdf, NCCN/cll_sll2025.pdf, NCCN/vulvar2024.pdf, NCCN/occult2025.pdf, NCCN/myeloma2025.pdf, NCCN/basal2025.pdf, NCCN/head-and-neck2025.pdf, NCCN/waldenstroms2025.pdf, NCCN/ovarian2025.pdf, NCCN/thyroid2025.pdf, NCCN/histiocytic_neoplasms2025.pdf, NCCN/breast2025.pdf, NCCN/mpn2025.pdf, NCCN/bone2025.pdf, NCCN/pancreatic2025.pdf, NCCN/ampullary2025.pdf, NCCN/mds2025.pdf, NCCN/hcc2025.pdf, NCCN/NHL_B-cell2025.pdf, NCCN/cns2025.pdf, NCCN/sarcoma2025.pdf, NCCN/all2024.pdf, NCCN/anal2025.pdf, NCCN/mcc2025.pdf, NCCN/colon2025.pdf, NCCN/testicular2025.pdf, NCCN/cervical2025.pdf, NCCN/kaposi2025.pdf, NCCN/uterine2025.pdf, NCCN/cutaneous_melanoma2025.pdf, NCCN/aml2025.pdf, NCCN/hodgkins2025.pdf, NCCN/vaginal2025.pdf, NCCN/neuroendocrine2025.pdf, NCCN/esophageal2025.pdf, NCCN/small_bowel2024.pdf, NCCN/kidney2025.pdf, NCCN/gastric2025.pdf, NCCN/uveal2025.pdf, NCCN/NHL_T-cell2025.pdf, NCCN/prostate2025.pdf, NCCN/thymic2025.pdf\n",
      "\n",
      "Documents that were chunked: 50\n",
      "Chunked sources: NCCN/cml2025.pdf, NCCN/gtn2025.pdf, NCCN/bladder2025.pdf, NCCN/sclc2025.pdf, NCCN/gist2025.pdf, NCCN/squamous2025.pdf, NCCN/nsclc2025.pdf, NCCN/rectal2025.pdf, NCCN/biliary2025.pdf, NCCN/cll_sll2025.pdf, NCCN/vulvar2024.pdf, NCCN/occult2025.pdf, NCCN/myeloma2025.pdf, NCCN/basal2025.pdf, NCCN/head-and-neck2025.pdf, NCCN/waldenstroms2025.pdf, NCCN/ovarian2025.pdf, NCCN/thyroid2025.pdf, NCCN/histiocytic_neoplasms2025.pdf, NCCN/breast2025.pdf, NCCN/mpn2025.pdf, NCCN/bone2025.pdf, NCCN/pancreatic2025.pdf, NCCN/ampullary2025.pdf, NCCN/mds2025.pdf, NCCN/hcc2025.pdf, NCCN/NHL_B-cell2025.pdf, NCCN/cns2025.pdf, NCCN/sarcoma2025.pdf, NCCN/all2024.pdf, NCCN/anal2025.pdf, NCCN/mcc2025.pdf, NCCN/colon2025.pdf, NCCN/testicular2025.pdf, NCCN/cervical2025.pdf, NCCN/kaposi2025.pdf, NCCN/uterine2025.pdf, NCCN/cutaneous_melanoma2025.pdf, NCCN/aml2025.pdf, NCCN/hodgkins2025.pdf, NCCN/vaginal2025.pdf, NCCN/neuroendocrine2025.pdf, NCCN/esophageal2025.pdf, NCCN/small_bowel2024.pdf, NCCN/kidney2025.pdf, NCCN/gastric2025.pdf, NCCN/uveal2025.pdf, NCCN/NHL_T-cell2025.pdf, NCCN/prostate2025.pdf, NCCN/thymic2025.pdf\n",
      "\n",
      "Files that failed to process:\n",
      "No failed files. All were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "# First, let's see what files we started with\n",
    "print(f\"Total PDFs found: {len(pdf_files)}\")\n",
    "print(f\"PDF files: {', '.join(pdf_files)}\")\n",
    "\n",
    "# After processing, let's see what made it into documents\n",
    "doc_sources = set(doc.metadata['source'] for doc in documents)\n",
    "print(f\"\\nSuccessfully processed documents: {len(doc_sources)}\")\n",
    "print(f\"Sources: {', '.join(doc_sources)}\")\n",
    "\n",
    "# After chunking, let's see what we have\n",
    "chunk_sources = set(chunk.metadata['source'] for chunk in chunks)\n",
    "print(f\"\\nDocuments that were chunked: {len(chunk_sources)}\")\n",
    "print(f\"Chunked sources: {', '.join(chunk_sources)}\")\n",
    "\n",
    "# Create sets of files\n",
    "original_files = set(pdf_files)\n",
    "processed_files = set(os.path.basename(source) for source in doc_sources)\n",
    "\n",
    "# Find failed files\n",
    "failed_files = original_files - processed_files\n",
    "print(\"\\nFiles that failed to process:\")\n",
    "if not failed_files:  # This checks if the set is empty\n",
    "    print(\"No failed files. All were successfully processed.\")\n",
    "else:\n",
    "    for file in failed_files:\n",
    "        print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "### Here each chunk of text is mapped into a vector that represents the meaning of the text. This is known as an embedding, and OpenAI's embedding method will be used. The vectors will be stored in a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 584/584 [18:38<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "def create_vectorstore_in_batches(chunks, embedding, batch_size=100, db_name=\"db\"):\n",
    "    total_batches = (len(chunks) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "    \n",
    "    # Initialize the vector store with the first batch\n",
    "    first_batch = chunks[:batch_size]\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=first_batch,\n",
    "        embedding=embedding,\n",
    "        persist_directory=db_name,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    \n",
    "    # Process the remaining chunks in batches\n",
    "    with tqdm(total=total_batches-1, desc=\"Processing batches\", ncols=150) as pbar:\n",
    "        for i in range(batch_size, len(chunks), batch_size):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            vectorstore.add_documents(documents=batch)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Use it like this:\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectorstore = create_vectorstore_in_batches(chunks, embeddings, batch_size=100, db_name=db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cdd0f2-aa03-4b94-8fa1-8baa730fed01",
   "metadata": {},
   "source": [
    "### Chroma creates collections in the vector database the holds information, such as the embeddings (vectors), associated metadata, documents/text, and IDs for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2e7687-60d4-4920-a1d7-a34b9f70a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 116,994 vectors with 3,072 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Create the chat model using OpenAI or a local open-source model. Create the retriever and conversation chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/jqsk5sjd6_95g21hssmwrx7r0000gn/T/ipykernel_29069/3145502156.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# Alternative - if you'd like to use Ollama locally, uncomment this line instead\n",
    "#llm = ChatOpenAI(temperature=0.7, model_name='llama3.3', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":5})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521bb61-ea4a-4f67-95f2-9fd1f27a9686",
   "metadata": {},
   "source": [
    "### Run a quick test with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "If a breast cancer patient fails fulvestrant, suitable options for next-line therapy may include:\n",
       "\n",
       "1. A CDK 4/6 inhibitor (such as palbociclib, ribociclib, or abemaciclib) in combination with another hormonal therapy, if not previously used.\n",
       "2. Everolimus in combination with either an aromatase inhibitor (AI), tamoxifen, or fulvestrant.\n",
       "3. Monotherapy with another hormonal therapy, depending on the specific clinical scenario.\n",
       "\n",
       "It's important to consider the patient's overall health, prior treatments, and specific cancer characteristics when deciding on the next-line therapy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "# Let's try a simple question\n",
    "\n",
    "query = \"If a breast cancer patient fails fulvestrant, what would be suitable options for next-line therapy?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "display(Markdown(result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438068fa-af3b-4f85-96a1-9198cfac7ee1",
   "metadata": {},
   "source": [
    "### Set up conversational memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b5a9013-d5d4-4e25-9e7c-cdbb4f33e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "### Create a chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a11e2-a4ca-44fc-bc2e-2165547b67ec",
   "metadata": {},
   "source": [
    "### Use Gradio for a chat user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, \n",
    "    type=\"messages\",\n",
    "    css=\"\"\"textarea {\n",
    "        font-size: 18px !important;\n",
    "    }\n",
    "    .message-wrap textarea {\n",
    "        font-size: 18px !important;\n",
    "    }\n",
    "    .message-content {\n",
    "        font-size: 18px !important;\n",
    "    }\n",
    "    .chatbot.prose {\n",
    "        font-size: 18px !important;\n",
    "    }\n",
    "    .svelte-7ddecg {\n",
    "        font-size 18px !important;\n",
    "    }\n",
    "    \"\"\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819970db-93b2-41fc-9193-959228f1c356",
   "metadata": {},
   "source": [
    "### This gives a behind-the-scenes peek into the \"thought process\" of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b55e9abb-e1da-46c5-acba-911868aee329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Stage I, centrale (T1abc–T2a, N0);  \n",
      "Stage II (T1abc–T2ab, N1; T2b, N0); \n",
      "Stage IIB (T3, N0)f; stage IIIA (T3, N1)f\n",
      "Stage IIB (T3 invasion, N0);\n",
      "Stage IIIA (T4 invasion, N0–1; \n",
      "T3, N1; T4, N0–1)\n",
      "Stage IIIA (T1–2, N2); stage IIIB (T3, N2) \n",
      "Separate pulmonary nodule(s) (stage IIB, IIIA, IV) \n",
      "Stage IIIA ipsilateral non-primary lobe (T4, N0–1); \n",
      "Stage IV (contralateral lung) \n",
      "Multiple lung cancers\n",
      "Stage IIIB (T1–2, N3); stage IIIC (T3, N3) \n",
      "Stage IIIB (T4, N2); stage IIIC (T4, N3)\n",
      "\n",
      "Stage I, centrale (T1abc–T2a, N0);  \n",
      "Stage II (T1abc–T2ab, N1; T2b, N0); \n",
      "Stage IIB (T3, N0)f; stage IIIA (T3, N1)f\n",
      "Stage IIB (T3 invasion, N0);\n",
      "Stage IIIA (T4 invasion, N0–1; \n",
      "T3, N1; T4, N0–1)\n",
      "Stage IIIA (T1–2, N2); stage IIIB (T3, N2) \n",
      "Separate pulmonary nodule(s) (stage IIB, IIIA, IV) \n",
      "Stage IIIA ipsilateral non-primary lobe (T4, N0–1); \n",
      "Stage IV (contralateral lung) \n",
      "Multiple lung cancers\n",
      "Stage IIIB (T1–2, N3); stage IIIC (T3, N3) \n",
      "Stage IIIB (T4, N2); stage IIIC (T4, N3)\n",
      "\n",
      "Stage IIIB and IIIC NSCLC  \n",
      "Stage IIIB NSCLC comprises two unresectable groups, including: 1) T1–\n",
      "2,N3 tumors; and 2) T3–4,N2 tumors; stage IIIC NSCLC includes T3,N3 \n",
      "and contralateral mediastinal nodes (T4,N3), which are also unresectable. \n",
      "Surgical resection is not recommended in patients with T1–2,N3 disease. \n",
      "However, in patients with suspected N3 disease, the NCCN Guidelines \n",
      "recommend pathologic confirmation of nodal status (see Pretreatment\n",
      "\n",
      "Stage IIIB and IIIC NSCLC  \n",
      "Stage IIIB NSCLC comprises two unresectable groups, including: 1) T1–\n",
      "2,N3 tumors; and 2) T3–4,N2 tumors; stage IIIC NSCLC includes T3,N3 \n",
      "and contralateral mediastinal nodes (T4,N3), which are also unresectable. \n",
      "Surgical resection is not recommended in patients with T1–2,N3 disease. \n",
      "However, in patients with suspected N3 disease, the NCCN Guidelines \n",
      "recommend pathologic confirmation of nodal status (see Pretreatment\n",
      "Human: What is stage is a T3,N1 lung cancer?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: A T3,N1 lung cancer is classified as stage IIIA.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "# Enter a simple sample query.\n",
    "query = \"What is stage is a T3,N1 lung cancer?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2136153b-d2f6-4c58-a0e3-78c3a932cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c2bfa3c-810b-441b-90d1-31533f14b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c736f33b-941e-4853-8eaf-2003bd988b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
